#  Football Match Outcome Prediction â€” Complete Machine Learning Project

A fully endâ€‘toâ€‘end, productionâ€‘ready **Football Match Outcome Prediction System** that uses advanced Machine Learning models like **CatBoost, XGBoost, and Random Forest**. This project predicts the match result â€” **Win / Draw / Loss** â€” using rich historical match data and powerful feature engineering.

This README is designed to be **GitHubâ€‘worthy**, clean, structured, and perfect for showcasing your project.

---

# ğŸ“Œ Project Summary

This project builds a machine learning model that predicts:

* **0 = Loss**
* **1 = Draw**
* **2 = Win**

Using:

* Historical match statistics
* Team performance trends
* Rolling form indicators
* Home/away advantage
* Season and competition data
* Advanced ML models

â†’ The best performing model was **CatBoostClassifier**, achieving **~73% accuracy**, which is strong for sports predictions.

---

# ğŸ“‚ Dataset Details

The dataset includes ~3800 matches with columns such as:

* Date
* Team
* Opponent
* Shots
* Shots on target
* Expected Goals (xG)
* Ball possession
* Formation
* Penalties
* Venue (Home/Away)
* Season
* Result (W/D/L)

After cleaning, the final working dataset contains **3102 matches**.

---

# ğŸ§¹ Data Cleaning & Preprocessing

### âœ” Dropped irrelevant columns:

* `notes`
* `referee`
* `match report`
* `time`

### âœ” Converted & sorted:

* Converted `date` â†’ datetime
* Sorted by `team + date` to prevent data leakage

### âœ” Removed leakage columns:

* `gf` (goals for)
* `ga` (goals against)
* `goal_diff`

These contain final results â†’ DO NOT USE as features.

### âœ” Null Handling

Initial cleaning removed rows with incomplete stats.

---

# ğŸ› ï¸ Feature Engineering (The Heart of the Project)

Feature engineering is essential in football analytics.

### ğŸ”µ Rolling Statistics (per team)

These capture a team's **form**:

* `Rolling Average 5 Sh` â€” avg shots over last 5 matches
* `rolling_avg_sot` â€” avg shots on target
* `rolling_avg_xg` â€” avg expected goals
* `Rolling win rate` â€” percent of recent wins

These features greatly improved predictions.

### ğŸ”µ Categorical Transformation

Converted match text info into numerical format:

* Competition
* Round
* Day
* Opponent
* Team
* Season

### ğŸ”µ Home Advantage

```
is_home = 1 if Home else 0
```

Home advantage is statistically significant.

### ğŸ”µ Final Feature Set

* Categorical (encoded)
* Numeric (scaled)
* Rolling stats
* Match context features

---

# ğŸ”¢ Encoding & Scaling

### Label Encoding

Used for categorical features:

* comp, round, day, opponent, team, season

### StandardScaler

Used for numerical features:

* xg, xga, sh, sot, dist, fk, pk, pkatt
* All rolling stats

CatBoost does not require encoding but dataset consistency is maintained.

---

# ğŸ¤– Machine Learning Models Used

### Models trained:

* **RandomForestClassifier**
* **XGBClassifier**
* **CatBoostClassifier** (BEST)
* Logistic Regression
* KNN

### Hyperparameter Tuning

Used:

* `GridSearchCV`
* `RandomizedSearchCV`

Best tuned parameters were obtained for XGBoost & CatBoost.

---

# ğŸ† Best Model: CatBoostClassifier

CatBoost performed best due to:

* Superior handling of categorical features
* Ordered boosting â†’ less overfitting
* Works well with tabular, mixed-type features
* Handles missing values internally
* Powerful for sports analytics

### ğŸ“ˆ Final Accuracy: **~73%**

Consistent across validation and test sets.

Draw prediction is naturally lower (common issue in football ML projects).

---

# ğŸ“Š Model Evaluation

### Metrics Used:

* Accuracy
* Precision
* Recall
* F1 Score
* Confusion Matrix

### Confusion Matrix Performance:

* Very strong for predicting **Win** and **Loss**
* Draw remains hardest (expected)

---

# ğŸš€ Deployment Preparation

To deploy the project, save:

### ğŸ”¹ Model

```
cat_model.save_model("model.cbm")
```

### ğŸ”¹ Encoders

```
joblib.dump(encoders, "encoders.pkl")
```

### ğŸ”¹ Scaler

```
joblib.dump(scaler, "scaler.pkl")
```

### ğŸ”¹ Final Features List

Required for prediction pipelines.

---

# ğŸ“ Project Folder Structure (Recommended)

```
project/
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ final_matches.csv
â”‚
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 1_raw_exploration.ipynb
â”‚ â”œâ”€â”€ 2_model_training.ipynb
â”‚ â””â”€â”€ 3_visualization.ipynb
â”‚
models/
â”‚ â”œâ”€â”€ catboost_model.cbm
â”‚ â”œâ”€â”€ rf_model.pkl
â”‚ â”œâ”€â”€ xgb_best.pkl
â”‚ â”œâ”€â”€ scaler.pkl
â”‚ â”œâ”€â”€ encoders.pkl
â”‚ â””â”€â”€ features.json
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

---

# ğŸ“¦ requirements.txt

```
pandas
numpy
scikit-learn
xgboost
catboost
matplotlib
seaborn
joblib
```

---

# ğŸ§  Key Learnings

* Prevent data leakage using proper sorting
* Use rolling windows for team form
* Encode categorical info carefully
* Scale numerical values
* CatBoost often wins tabular ML problems
* Draw classes are always hardest
* Hyperparameter tuning improves stability
* Save preprocessing artifacts for deployment

---

# âœ¨ Author

**Vinayak Tanwar**
Machine Learning & Data Science Enthusiast

If you like this project, â­ star the repository on GitHub!
